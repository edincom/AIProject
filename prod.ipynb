{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e5d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API KEY information\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5518043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters and ChatMistral object creation\n",
    "\n",
    "\n",
    "# Create the ChatMistralAI object\n",
    "llm = ChatMistralAI(\n",
    "    temperature=1,  # Low temperature for more focused responses\n",
    "    model=\"mistral-small-latest\", \n",
    ")\n",
    "\n",
    "#If we want to understand pictures, we should use this model : \"pixtral-12b-2409\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f74b0f",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84096225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the document: 308\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Documents\n",
    "loader = PyMuPDFLoader(\"Data/Atlas.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of pages in the document: {len(docs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58adbe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split chunks: 696\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split Documents\n",
    "custom_separators = [\n",
    "    \"\\n \\n\",        # paragraphs\n",
    "    \"\\n\",         # lines\n",
    "    \". \",         # sentence-ish boundary\n",
    "    \"; \",         # clause boundary\n",
    "    \", \",         # phrase boundary\n",
    "    \" \",          # words\n",
    "    \"\"            # fallback: characters\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = custom_separators, chunk_size=500, chunk_overlap=50)      #Param√®tre √† modifier par la suite pour de meilleur performance\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"Number of split chunks: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b469c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate Embeddings\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc73c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create and Save the Database\n",
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a7523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create Retriever\n",
    "# Search and retrieve information contained in the documents\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b39fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create Prompt\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb16f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Setup LLM\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f6c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Create Chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bd5bb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je ne sais pas.\n",
      "-----\n",
      "Selon le contexte fourni, l'Islande est le premier pays √† avoir l√©galis√© l'avortement en 1934.\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "# Run Chain\n",
    "# Input a query about the document and print the response\n",
    "question = \"Qui a gagn√© le ballon d'or en 2009\"\n",
    "response = chain.invoke(question)\n",
    "print(response)\n",
    "print(\"-----\")\n",
    "question = \"Quel pays a l√©galis√© l'avortement en premier dans le monde ?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d87cd",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n",
    "Here we setup a basic template for our prompt engineering.\n",
    "In our case, the LLM will be a specialist in geography, in secondary school.\n",
    "\n",
    "The student will interact with the LLM in two different ways :\n",
    "    -He can ask any type of question about any topic in the course.\n",
    "    -He can ask to have his knowledge tested (he will then receive a score on his answer and a feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a7c978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persona prompt, specific for when the student has a question about a specific part of the course\n",
    "\n",
    "persona_template = (\n",
    "    \"Act as a supportive but rigorous geography teacher.\\n\" \\\n",
    "    \"Your tone should be constructive, specific, and pedagogical.\\n\" \\\n",
    "        \"\"\"Tu es un professeur de g√©ographie avec 20 ans d'exp√©rience, et ton but est de r√©pondre aux questions d'un √©l√®ve en difficult√©.\n",
    "            Tu es encourageant, mais tout de fois rigoureux quant √† la pr√©cision de tes r√©ponses.\n",
    "\n",
    "    CONTRAINTES:\n",
    "    1. Utilise UNIQUEMENT le contexte fourni.\n",
    "    2. Cite chaque fait avec la page sous forme [Page X].\n",
    "    3. Ne fabrique rien.\n",
    "\n",
    "    Format attendu:\n",
    "    R√©ponse concise en fran√ßais.\n",
    "    CITES: Page: X,Y,... (liste unique de pages utilis√©es)\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Contexte:\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "scores = \"\"\"\n",
    "- Pertinence : Est-ce que l'√©tudiant r√©pond bien √† la question pos√© et non pas √† autre chose  /30;\n",
    "- Faits non correctes: Est-ce qu'il y'a des faits qui ne sont pas correctes dans la r√©ponse  /30;\n",
    "- Faits manquants : Est-ce que tous les faits attendus sont bien pr√©sent dans la r√©ponse  /30;\n",
    "- Stucture : Est-ce que la r√©ponse est bien stuctur√©e /10;\n",
    "\"\"\"\n",
    "\n",
    "test_template = (    f\"Act as a supportive but rigorous history teacher.\\n\"\n",
    "    \"Your goal is to generate a question based on the course.\"             \n",
    "    \"The student gives you an answer and your goal is to evaluate it.\\n\"\n",
    "    \"Assignment requirement: {task_description}\\n\"\n",
    "    \"Grading rubric: {grading_rubric}\\n\"\n",
    "    \"Return ONLY a JSON object with these keys:\\n\"\n",
    "    \"- Section: the general theme of the question\\n\"\n",
    "    \"- Question: the question you asked the student\\n\"\n",
    "    \"- Answer: The answer the student gave\\n\"\n",
    "    \"- grade: number (0-100), must equal sum of all scores\\n\"\n",
    "    \"- scores: f{scores} \\n\"\n",
    "    \"- advice: array of short, actionable improvement suggestions\\n\"\n",
    "    \"Constraints: grade MUST equal Pertinence+Faits non correctes + Faits manquants + Structure. No extra text outside the JSON.\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8370eff",
   "metadata": {},
   "source": [
    "# Data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f807f484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database 'student_results.db' ready\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Create the SQLite database\n",
    "# -------------------------------\n",
    "conn = sqlite3.connect('student_results.db')  # This creates a file on disk\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create table for storing answers and grading\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS student_results (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    student_name TEXT,\n",
    "    question TEXT,\n",
    "    answer TEXT,\n",
    "    grade REAL,\n",
    "    scores TEXT,        -- we'll store JSON as a string\n",
    "    advice TEXT\n",
    ")\n",
    "''')\n",
    "conn.commit()\n",
    "print(\"Database 'student_results.db' ready\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Function to save a result\n",
    "# -------------------------------\n",
    "def save_result(student_name, question, answer, grading_json):\n",
    "    \"\"\"\n",
    "    grading_json: dictionary returned by LLM in test mode\n",
    "    \"\"\"\n",
    "    # Convert the 'scores' dict to a JSON string\n",
    "    scores_str = json.dumps(grading_json.get(\"scores\", {}), ensure_ascii=False)\n",
    "\n",
    "    cursor.execute('''\n",
    "        INSERT INTO student_results (student_name, question, answer, grade, scores, advice)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "    ''', (\n",
    "        student_name,\n",
    "        question,\n",
    "        answer,\n",
    "        grading_json.get(\"grade\", 0),\n",
    "        scores_str,\n",
    "        grading_json.get(\"advice\", \"\")\n",
    "    ))\n",
    "    conn.commit()\n",
    "    print(f\"Result for {student_name} saved successfully!\")\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Example usage\n",
    "# -------------------------------\n",
    "\n",
    "# Suppose we have a grading result from the LLM (already parsed as JSON)\n",
    "example_grading_json = {\n",
    "    \"Section\": \"Histoire\",\n",
    "    \"Question\": \"Quels furent les principaux √©v√©nements qui ont marqu√© le d√©but de la Seconde Guerre mondiale en Europe ?\",\n",
    "    \"Answer\": \"L'invasion de la Pologne par l'Allemagne, et le fait qu'il y'avait une crise √©conomique assez forte\",\n",
    "    \"grade\": 40,\n",
    "    \"scores\": {\n",
    "        \"Pertinence\": 20,\n",
    "        \"Faits non correctes\": 20,\n",
    "        \"Faits manquants\": 40,\n",
    "        \"Structure\": 10\n",
    "    },\n",
    "    \"advice\": \"La r√©ponse mentionne correctement l'invasion de la Pologne, qui est un √©v√©nement cl√©. Cependant, la crise √©conomique, bien que pertinente, n'est pas un √©v√©nement marquant du d√©but de la guerre...\"\n",
    "}\n",
    "\n",
    "#save_result(\"Edin\", example_grading_json[\"Question\"], example_grading_json[\"Answer\"], example_grading_json)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760ad84e",
   "metadata": {},
   "source": [
    "# Vector embeddings\n",
    "\n",
    "Only needs to be run one to get the vectors. They are stored in a folder called \"faiss_store\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbeea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF...\n",
      "‚úÖ Loaded 308 pages\n",
      "\n",
      "üîç Analyzing document structure...\n",
      "\n",
      "============================================================\n",
      "üìñ DOCUMENT STRUCTURE\n",
      "============================================================\n",
      "```json\n",
      "{\n",
      "  \"title\": \"Grand Atlas 2024\",\n",
      "  \"chapters\": [\n",
      "    {\n",
      "      \"number\": \"1\",\n",
      "      \"title\": \"Points chauds\",\n",
      "      \"subsections\": [\n",
      "        \"La guerre en Ukraine : la Russie peut-elle gagner ?\",\n",
      "        \"L‚ÄôIndopacifique, th√©√¢tre des rivalit√©s mondiales\",\n",
      "        \"Ta√Øwan : des √©lections √† haut risque\",\n",
      "        \"Turquie : la victoire d‚ÄôErdogan\",\n",
      "        \"Le Caucase sous tensions\",\n",
      "        \"Syrie, une guerre inachev√©e ?\",\n",
      "        \"Le Kosovo : un anniversaire sous tension\",\n",
      "        \"Un conflit isra√©lo-palestinien loin d‚Äô√™tre marginalis√©\",\n",
      "        \"Iran, entre r√©pression et isolement\",\n",
      "        \"L‚Äôinstabilit√© gagne-t-elle l‚ÄôAfrique ?\",\n",
      "        \"Un arc de crise sah√©lien\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"2\",\n",
      "      \"title\": \"Les grands enjeux de 2024\",\n",
      "      \"subsections\": [\n",
      "        \"L‚ÄôOTAN, le retour, 75 ans apr√®s sa naissance\",\n",
      "        \"Les √âtats-Unis, de retour dans les affaires du monde\",\n",
      "        \"Une Am√©rique latine plus stable politiquement ?\",\n",
      "        \"Les d√©mocraties au risque de la d√©sinformation\",\n",
      "        \"La dissuasion nucl√©aire a-t-elle encore un sens ?\",\n",
      "        \"√âradiquer la faim, un d√©sir sans fin\",\n",
      "        \"Des guerres pour les ressources\",\n",
      "        \"Le droit √† l‚Äôavortement menac√© dans le monde ?\",\n",
      "        \"R√©sorber les in√©galit√©s : une gageure\",\n",
      "        \"2024 : Paris accueille les Jeux olympiques\",\n",
      "        \"Les JO, instruments du soft power des √âtats\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"3\",\n",
      "      \"title\": \"Retour sur l‚Äôhistoire\",\n",
      "      \"subsections\": [\n",
      "        \"Il y a 230 ans, l‚Äôesclavage est aboli\",\n",
      "        \"Il y a 210 ans, un nouvel ordre pour l‚ÄôEurope\",\n",
      "        \"Il y a 200 ans, la fin de l‚ÄôEmpire colonial espagnol\",\n",
      "        \"Il y a 130 ans, la renaissance des Jeux olympiques\",\n",
      "        \"Il y a 80 ans, Paris lib√©r√©\",\n",
      "        \"Il y a 55 ans, on a march√© sur la Lune\",\n",
      "        \"Il y a 25 ans, la naissance de l‚Äôeuro\",\n",
      "        \"Il y a 10 ans, l‚Äôannexion de la Crim√©e par la Russie\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"4\",\n",
      "      \"title\": \"Et demain ?\",\n",
      "      \"subsections\": [\n",
      "        \"10 milliards d‚Äôhumains en 2050\",\n",
      "        \"L‚Äôhumanit√© concentr√©e dans les pays du Sud\",\n",
      "        \"Faire reculer la mort\",\n",
      "        \"De l‚Äôurgence d‚Äôune transition √©nerg√©tique\",\n",
      "        \"Vivre la ville diff√©remment\",\n",
      "        \"Un monde sans voitures\",\n",
      "        \"Cultiver autrement\",\n",
      "        \"Une agriculture plus bio\",\n",
      "        \"Vers une disparition et migration des for√™ts ?\",\n",
      "        \"Prot√©ger la mer et les oc√©ans\",\n",
      "        \"La g√©oing√©nierie : une solution au r√©chauffement ?\",\n",
      "        \"L‚ÄôInde, premi√®re puissance d√©mographique : atout ou fardeau ?\",\n",
      "        \"Un moment crucial pour l‚ÄôEurope spatiale\"\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"number\": \"5\",\n",
      "      \"title\": \"Dossier sp√©cial\",\n",
      "      \"subsections\": [\n",
      "        \"Crises, g√©rer l‚Äôurgence ?\",\n",
      "        \"Crises et risques croissants\",\n",
      "        \"Catastrophes naturelles : de l‚Äôal√©a au risque\",\n",
      "        \"La for√™t en crise\",\n",
      "        \"Les risques li√©s √† l‚Äôeau\",\n",
      "        \"Des risques √©pid√©miques croissants\",\n",
      "        \"Des in√©galit√©s face aux risques\",\n",
      "        \"Des risques amplifi√©s par le r√©chauffement climatique\",\n",
      "        \"La France face aux risques : quelle(s) politique(s) ?\",\n",
      "        \"La biodiversit√© menac√©e par l‚Äô√©conomie du sexe\",\n",
      "        \"R√©chauffement climatique, la porte ouverte aux √©pid√©mies\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"additional_sections\": [\n",
      "    \"√âdito\",\n",
      "    \"Pr√©face\",\n",
      "    \"Postface\",\n",
      "    \"Annexes\",\n",
      "    \"√Ä vous de jouer\",\n",
      "    \"Sources et cr√©dits\"\n",
      "  ]\n",
      "}\n",
      "```\n",
      "\n",
      "üéØ Extracting themes from document...\n",
      "\n",
      "============================================================\n",
      "üé® MAIN THEMES\n",
      "============================================================\n",
      "```json\n",
      "{\n",
      "  \"main_themes\": [\n",
      "    \"Latin American Independence Movements\",\n",
      "    \"Environmental Technologies and Risks\",\n",
      "    \"Access to Knowledge and Culture\"\n",
      "  ],\n",
      "  \"key_topics\": [\n",
      "    \"Historical Events and Figures in Latin American Independence\",\n",
      "    \"Environmental Impact and Future Generations\",\n",
      "    \"Digital Libraries and Access to Information\"\n",
      "  ],\n",
      "  \"document_type\": \"textbook\",\n",
      "  \"summary\": \"The document covers a range of topics including the historical events and key figures involved in the independence movements of Latin American countries, the potential risks and unknowns of certain environmental technologies, and the importance of accessible knowledge and culture through digital libraries.\"\n",
      "}\n",
      "```\n",
      "\n",
      "üìö Building vector store...\n",
      "‚úÖ Created 429 chunks\n",
      "\n",
      "============================================================\n",
      "‚úÖ RAG SYSTEM READY\n",
      "============================================================\n",
      "\n",
      "You can now query specific chapters for more details!\n",
      "\n",
      "üíæ Results saved to 'document_analysis.txt'\n",
      "\n",
      "============================================================\n",
      "USAGE EXAMPLES\n",
      "============================================================\n",
      "\n",
      "# Query a specific chapter:\n",
      "details = query_chapter_details(retriever, llm, 'Introduction')\n",
      "\n",
      "# Query a theme:\n",
      "details = query_chapter_details(retriever, llm, 'Risk Management')\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PDF Chapter & Theme Extractor using RAG\n",
    "Extracts all chapters and themes from a PDF document\n",
    "\"\"\"\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import json\n",
    "\n",
    "# ============================================================\n",
    "# 1. SETUP LLM\n",
    "# ============================================================\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",\n",
    "    temperature=0  # Low temperature for structured extraction\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD PDF\n",
    "# ============================================================\n",
    "\n",
    "print(\"Loading PDF...\")\n",
    "loader = PyMuPDFLoader(\"Data/Atlas.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"‚úÖ Loaded {len(docs)} pages\")\n",
    "\n",
    "# ============================================================\n",
    "# 3. EXTRACT TABLE OF CONTENTS & STRUCTURE\n",
    "# ============================================================\n",
    "\n",
    "def extract_chapters_and_themes(docs, llm):\n",
    "    \"\"\"\n",
    "    Extract chapters and themes from PDF using RAG approach\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get first 20 pages (usually contains TOC and intro)\n",
    "    first_pages = docs[:20]\n",
    "    first_pages_text = \"\\n\\n\".join([doc.page_content for doc in first_pages])\n",
    "    \n",
    "    # Prompt to extract TOC\n",
    "    toc_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert at analyzing document structure. \n",
    "Extract the table of contents, chapters, and main sections from the document.\n",
    "\n",
    "Return a JSON structure like this:\n",
    "{{\n",
    "  \"title\": \"Document Title\",\n",
    "  \"chapters\": [\n",
    "    {{\n",
    "      \"number\": \"1\",\n",
    "      \"title\": \"Chapter Title\",\n",
    "      \"page\": 10,\n",
    "      \"subsections\": [\"Subsection 1\", \"Subsection 2\"]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "If no clear chapter structure exists, identify the main sections and themes.\"\"\"),\n",
    "        (\"user\", \"Here are the first pages of the document:\\n\\n{text}\\n\\nExtract the structure:\")\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüîç Analyzing document structure...\")\n",
    "    chain = toc_prompt | llm | StrOutputParser()\n",
    "    toc_result = chain.invoke({\"text\": first_pages_text[:15000]})  # Limit to avoid token limits\n",
    "    \n",
    "    return toc_result\n",
    "\n",
    "# ============================================================\n",
    "# 4. EXTRACT THEMES FROM FULL DOCUMENT\n",
    "# ============================================================\n",
    "\n",
    "def extract_themes_from_full_doc(docs, llm):\n",
    "    \"\"\"\n",
    "    Extract main themes by analyzing the entire document\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample pages throughout the document\n",
    "    sample_indices = [0, len(docs)//4, len(docs)//2, 3*len(docs)//4, len(docs)-1]\n",
    "    sample_pages = [docs[i] for i in sample_indices if i < len(docs)]\n",
    "    sample_text = \"\\n\\n\".join([f\"[Page {doc.metadata['page']}]\\n{doc.page_content}\" for doc in sample_pages])\n",
    "    \n",
    "    themes_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are an expert at thematic analysis. \n",
    "Analyze the document and identify the main themes, topics, and key concepts.\n",
    "\n",
    "Return a JSON structure like this:\n",
    "{{\n",
    "  \"main_themes\": [\"Theme 1\", \"Theme 2\", \"Theme 3\"],\n",
    "  \"key_topics\": [\"Topic 1\", \"Topic 2\"],\n",
    "  \"document_type\": \"textbook/manual/report/etc\",\n",
    "  \"summary\": \"Brief overview of what the document covers\"\n",
    "}}\"\"\"),\n",
    "        (\"user\", \"Here are sample pages from throughout the document:\\n\\n{text}\\n\\nExtract the themes:\")\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüéØ Extracting themes from document...\")\n",
    "    chain = themes_prompt | llm | StrOutputParser()\n",
    "    themes_result = chain.invoke({\"text\": sample_text[:15000]})\n",
    "    \n",
    "    return themes_result\n",
    "\n",
    "# ============================================================\n",
    "# 5. CREATE RAG SYSTEM FOR DETAILED QUERIES\n",
    "# ============================================================\n",
    "\n",
    "def create_rag_system(docs):\n",
    "    \"\"\"\n",
    "    Create a RAG system to answer specific questions about chapters\n",
    "    \"\"\"\n",
    "    print(\"\\nüìö Building vector store...\")\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "    \n",
    "    splits = text_splitter.split_documents(docs)\n",
    "    print(f\"‚úÖ Created {len(splits)} chunks\")\n",
    "    \n",
    "    embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "    \n",
    "    return retriever\n",
    "\n",
    "# ============================================================\n",
    "# 6. QUERY SPECIFIC CHAPTERS\n",
    "# ============================================================\n",
    "\n",
    "def query_chapter_details(retriever, llm, chapter_name):\n",
    "    \"\"\"\n",
    "    Get detailed information about a specific chapter\n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve relevant chunks\n",
    "    docs = retriever.invoke(f\"What is covered in {chapter_name}?\")\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    detail_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are analyzing a document chapter. Provide a detailed summary of the content.\"),\n",
    "        (\"user\", \"\"\"Based on this context about {chapter}:\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Provide:\n",
    "        1. Main topics covered\n",
    "        2. Key concepts\n",
    "        3. Important details or findings\"\"\")\n",
    "            ])\n",
    "    \n",
    "    chain = detail_prompt | llm | StrOutputParser()\n",
    "    result = chain.invoke({\"chapter\": chapter_name, \"context\": context})\n",
    "    \n",
    "    return result\n",
    "\n",
    "# ============================================================\n",
    "# 7. MAIN EXECUTION\n",
    "# ============================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to extract all chapters and themes\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract structure\n",
    "    structure = extract_chapters_and_themes(docs, llm)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üìñ DOCUMENT STRUCTURE\")\n",
    "    print(\"=\"*60)\n",
    "    print(structure)\n",
    "    \n",
    "    # Extract themes\n",
    "    themes = extract_themes_from_full_doc(docs, llm)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üé® MAIN THEMES\")\n",
    "    print(\"=\"*60)\n",
    "    print(themes)\n",
    "    \n",
    "    # Create RAG system for detailed queries\n",
    "    retriever = create_rag_system(docs)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úÖ RAG SYSTEM READY\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nYou can now query specific chapters for more details!\")\n",
    "    \n",
    "    # Example: Query first chapter (uncomment to use)\n",
    "    # print(\"\\n\" + \"=\"*60)\n",
    "    # print(\"üìù EXAMPLE: Chapter 1 Details\")\n",
    "    # print(\"=\"*60)\n",
    "    # chapter_details = query_chapter_details(retriever, llm, \"Chapter 1\")\n",
    "    # print(chapter_details)\n",
    "    \n",
    "    return structure, themes, retriever\n",
    "\n",
    "# ============================================================\n",
    "# RUN\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    structure, themes, retriever = main()\n",
    "    \n",
    "    # Optional: Save results to file\n",
    "    with open(\"document_analysis.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"DOCUMENT STRUCTURE\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(structure + \"\\n\\n\")\n",
    "        f.write(\"MAIN THEMES\\n\")\n",
    "        f.write(\"=\"*60 + \"\\n\")\n",
    "        f.write(themes + \"\\n\")\n",
    "    \n",
    "    print(\"\\nüíæ Results saved to 'document_analysis.txt'\")\n",
    "    \n",
    "    # Example usage for querying specific chapters:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"USAGE EXAMPLES\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n# Query a specific chapter:\")\n",
    "    print(\"details = query_chapter_details(retriever, llm, 'Introduction')\")\n",
    "    print(\"\\n# Query a theme:\")\n",
    "    print(\"details = query_chapter_details(retriever, llm, 'Risk Management')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56125cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_mistralai.chat_models.ChatMistralAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised ReadTimeout: The read operation timed out.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    112\u001b[39m     pdf.close()\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m image_docs\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m image_docs = \u001b[43mcaption_images_from_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mData/Atlas.pdf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m all_docs = text_docs + image_docs\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# 4. EMBEDDINGS + VECTORSTORE + RETRIEVER\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mcaption_images_from_pdf\u001b[39m\u001b[34m(pdf_path)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[32m     87\u001b[39m message = HumanMessage(\n\u001b[32m     88\u001b[39m     content=[\n\u001b[32m     89\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDescribe this image in detailed natural language.\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m   (...)\u001b[39m\u001b[32m     94\u001b[39m     ]\n\u001b[32m     95\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m response = \u001b[43mvision_llm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     98\u001b[39m caption = response.content\n\u001b[32m    100\u001b[39m image_docs.append(\n\u001b[32m    101\u001b[39m     Document(\n\u001b[32m    102\u001b[39m         page_content=caption,\n\u001b[32m   (...)\u001b[39m\u001b[32m    109\u001b[39m     )\n\u001b[32m    110\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:631\u001b[39m, in \u001b[36mChatMistralAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[39m\n\u001b[32m    629\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    630\u001b[39m params = {**params, **kwargs}\n\u001b[32m--> \u001b[39m\u001b[32m631\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:555\u001b[39m, in \u001b[36mChatMistralAI.completion_with_retry\u001b[39m\u001b[34m(self, run_manager, **kwargs)\u001b[39m\n\u001b[32m    552\u001b[39m     _raise_on_error(response)\n\u001b[32m    553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m--> \u001b[39m\u001b[32m555\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_completion_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_mistralai\\chat_models.py:551\u001b[39m, in \u001b[36mChatMistralAI.completion_with_retry.<locals>._completion_with_retry\u001b[39m\u001b[34m(**kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m                 \u001b[38;5;28;01myield\u001b[39;00m event.json()\n\u001b[32m    550\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m iter_sse()\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m _raise_on_error(response)\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:1144\u001b[39m, in \u001b[36mClient.post\u001b[39m\u001b[34m(self, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1124\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1125\u001b[39m     url: URL | \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1137\u001b[39m     extensions: RequestExtensions | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1138\u001b[39m ) -> Response:\n\u001b[32m   1139\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1140\u001b[39m \u001b[33;03m    Send a `POST` request.\u001b[39;00m\n\u001b[32m   1141\u001b[39m \n\u001b[32m   1142\u001b[39m \u001b[33;03m    **Parameters**: See `httpx.request`.\u001b[39;00m\n\u001b[32m   1143\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1145\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1146\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcookies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpx\\_transports\\default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_sync\\http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, max_bytes: \u001b[38;5;28mint\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mbytes\u001b[39m:\n\u001b[32m    125\u001b[39m     exc_map: ExceptionMapping = {socket.timeout: ReadTimeout, \u001b[38;5;167;01mOSError\u001b[39;00m: ReadError}\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmap_exceptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43msettimeout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, typ, value, traceback):\n\u001b[32m    142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL WORKING VERSION ‚Äî RAG + PERSONA CHAIN + GRADING CHAIN\n",
    "# ============================================================\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_community.docstore.document import Document\n",
    "import base64\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. LLM CONFIG\n",
    "# ============================================================\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",   # Vision model would be: pixtral-12b-2409\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "vision_llm = ChatMistralAI(\n",
    "    model=\"pixtral-12b-2409\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# OPTIONAL: if you want vision, replace above with:\n",
    "# llm = ChatMistralAI(model=\"pixtral-12b-2409\", temperature=1)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD DOCUMENTS (PDF)\n",
    "# ============================================================\n",
    "\n",
    "loader = PyMuPDFLoader(\"Data/Atlas.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. SPLIT DOCUMENTS\n",
    "# ============================================================\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    #separators=[\"\\nCHAPITRE\", \"\\n##\", \"\\n###\", \"\\nSection\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "text_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "def caption_images_from_pdf(pdf_path):\n",
    "    pdf = fitz.open(pdf_path)\n",
    "    image_docs = []\n",
    "\n",
    "    for page_num in range(len(pdf)):\n",
    "        page = pdf.load_page(page_num)\n",
    "\n",
    "        for img_index, img_info in enumerate(page.get_images(full=True)):\n",
    "            xref = img_info[0]\n",
    "            image_data = pdf.extract_image(xref)\n",
    "\n",
    "            image_bytes = image_data[\"image\"]\n",
    "            ext = image_data.get(\"ext\", \"png\")\n",
    "\n",
    "            # Base64 encode\n",
    "            raw_b64 = base64.b64encode(image_bytes).decode(\"utf-8\")\n",
    "\n",
    "            mime_map = {\n",
    "                \"png\": \"image/png\",\n",
    "                \"jpg\": \"image/jpeg\",\n",
    "                \"jpeg\": \"image/jpeg\"\n",
    "            }\n",
    "            mime = mime_map.get(ext.lower(), \"image/png\")\n",
    "\n",
    "            # Correct format for Pixtral/LangChain\n",
    "            image_url = f\"data:{mime};base64,{raw_b64}\"\n",
    "\n",
    "            # CORRECTED: Use proper message format for vision models\n",
    "            from langchain_core.messages import HumanMessage\n",
    "            \n",
    "            message = HumanMessage(\n",
    "                content=[\n",
    "                    {\"type\": \"text\", \"text\": \"Describe this image in detailed natural language.\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": image_url}\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            response = vision_llm.invoke([message])\n",
    "            caption = response.content\n",
    "\n",
    "            image_docs.append(\n",
    "                Document(\n",
    "                    page_content=caption,\n",
    "                    metadata={\n",
    "                        \"type\": \"image_caption\",\n",
    "                        \"page\": page_num + 1,\n",
    "                        \"image_index\": img_index,\n",
    "                        \"source\": pdf_path\n",
    "                    }\n",
    "                )\n",
    "            )\n",
    "\n",
    "    pdf.close()\n",
    "    return image_docs\n",
    "\n",
    "\n",
    "image_docs = caption_images_from_pdf(\"Data/Atlas.pdf\")\n",
    "all_docs = text_docs + image_docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. EMBEDDINGS + VECTORSTORE + RETRIEVER\n",
    "# ============================================================\n",
    "\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(all_docs, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Save to disk\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "print(\"Vectorstore saved to 'faiss_index/'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d37642",
   "metadata": {},
   "source": [
    "# Interface\n",
    "\n",
    "There are basically two ways the student can interact with the LLM : there is the mode \"teaching\" where the student asks question about the course in order to enhance his knowledge and understanding of the subject, and there is the mode \"test\" where the LLM generates a question based on the course and is the student is graded based on the quality of his answer. The grade shall then be stored in a local db to track the student's progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e546a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore loaded successfully\n",
      "=== Tutor System Running ===\n",
      "Ask a question to the teacher, or type 'test' to grade an answer.\n",
      "\n",
      "\n",
      "--- Question g√©n√©r√©e automatiquement ---\n",
      "**Question :** En vous basant sur le contexte fourni, expliquez comment la course √† l'espace, initi√©e par le lancement du premier satellite artificiel sovi√©tique (Spoutnik) en 1957, a influenc√© les dynamiques de la Guerre froide. Quels √©taient les enjeux politiques, technologiques et id√©ologiques de cette comp√©tition spatiale entre les √âtats-Unis et l'Union sovi√©tique ?\n",
      "\n",
      "--- JSON Grading Result ---\n",
      "{'Section': 'Guerre froide', 'Question': \"En vous basant sur le contexte fourni, expliquez comment la course √† l'espace, initi√©e par le lancement du premier satellite artificiel sovi√©tique (Spoutnik) en 1957, a influenc√© les dynamiques de la Guerre froide. Quels √©taient les enjeux politiques, technologiques et id√©ologiques de cette comp√©tition spatiale entre les √âtats-Unis et l'Union sovi√©tique ?\", 'Answer': 'Je sais pas trop', 'grade': 0, 'scores': {'Pertinence': 0, 'Faits non correctes': 0, 'Faits manquants': 30, 'Structure': 0}, 'advice': \"Il est important de fournir une r√©ponse d√©taill√©e qui aborde les aspects politiques, technologiques et id√©ologiques de la course √† l'espace pendant la Guerre froide. Assurez-vous de mentionner des faits sp√©cifiques et de structurer votre r√©ponse de mani√®re coh√©rente.\"}\n",
      "\n",
      "Note : 0\n",
      "Conseils : Il est important de fournir une r√©ponse d√©taill√©e qui aborde les aspects politiques, technologiques et id√©ologiques de la course √† l'espace pendant la Guerre froide. Assurez-vous de mentionner des faits sp√©cifiques et de structurer votre r√©ponse de mani√®re coh√©rente.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_mistralai import MistralAIEmbeddings\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "\n",
    "# Load the vectorstore (allow unsafe deserialization since you created it)\n",
    "vectorstore = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "print(\"Vectorstore loaded successfully\")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. PERSONA CHAIN (Teacher mode + RAG)\n",
    "# ============================================================\n",
    "\n",
    "persona_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Tu es un professeur bienveillant. Explique simplement mais sans infantiliser. \"\n",
    "     \"Appuie-toi uniquement sur le contexte fourni.\"),\n",
    "    (\"human\",\n",
    "     \"Question: {question}\\n\\n\"\n",
    "     \"Contexte issu des documents:\\n{context}\")\n",
    "])\n",
    "\n",
    "persona_chain = (\n",
    "    persona_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. GRADING CHAIN (Automatic evaluation)\n",
    "# ============================================================\n",
    "\n",
    "test_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Tu es un correcteur automatique. √âvalue la r√©ponse de l'√©l√®ve selon les crit√®res fournis.\"),\n",
    "    (\"human\",\n",
    "     #\"Instruction donn√©e √† l'√©l√®ve : {task_description}\\n\"\n",
    "     \"Bar√®me : {grading_rubric}\\n\"\n",
    "     \"Question : {question}\\n\"\n",
    "     \"R√©ponse de l'√©l√®ve : {answer}\\n\\n\"\n",
    "     \"Donne une note sur 20 + justification.\")\n",
    "])\n",
    "\n",
    "test_chain = (\n",
    "    test_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. RAG WRAPPER ‚Äî run retrieval only if needed\n",
    "# ============================================================\n",
    "\n",
    "def rag_logic(inputs):\n",
    "    query = inputs.get(\"question\", \"\")\n",
    "    if not query:\n",
    "        inputs[\"context\"] = \"\"\n",
    "        return inputs\n",
    "\n",
    "    docs = retriever.invoke(query)\n",
    "    ctx = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    inputs[\"context\"] = ctx\n",
    "    return inputs\n",
    "\n",
    "\n",
    "rag_chain = RunnableLambda(rag_logic)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. ROUTING ‚Äî decide whether to use Persona or Test chain\n",
    "# ============================================================\n",
    "\n",
    "def route(inputs):\n",
    "    \"\"\"\n",
    "    If 'answer' is provided ‚Üí grading mode.\n",
    "    Otherwise ‚Üí persona/teacher mode.\n",
    "    \"\"\"\n",
    "    return \"answer\" not in inputs\n",
    "\n",
    "\n",
    "conditional_chain = RunnableBranch(\n",
    "    # condition ‚Üí persona mode\n",
    "    (\n",
    "        lambda inputs: route(inputs),\n",
    "        rag_chain | persona_chain\n",
    "    ),\n",
    "    # fallback ‚Üí grading mode\n",
    "    test_template | llm\n",
    ")\n",
    "\n",
    "def generate_test_question(criteria):\n",
    "    \"\"\"\n",
    "    Generate a test question using document context (RAG) and student instructions.\n",
    "    \"\"\"\n",
    "    # Step 1: Retrieve relevant context using RAG\n",
    "    # Use invoke() method to retrieve documents\n",
    "    docs = retriever.invoke(criteria)               #Here criteria means the subject the students wishes to be tested in.\n",
    "\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "    # Step 2: Prompt for question generation using the retrieved context\n",
    "    question_gen_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\",\n",
    "         \"Tu es un professeur bienveillant et rigoureux. \"\n",
    "         \"√Ä partir du contexte fourni, g√©n√®re une question pertinente pour un √©l√®ve.\"),\n",
    "        (\"human\",\n",
    "         \"Instructions : {criteria}\\n\\nContexte : {context}\")\n",
    "    ])\n",
    "\n",
    "    question_gen_chain = question_gen_prompt | llm\n",
    "\n",
    "    generated_question = question_gen_chain.invoke({\n",
    "        \"criteria\": criteria,\n",
    "        \"context\": context_text\n",
    "    }).content.strip()\n",
    "\n",
    "    return generated_question\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. MAIN ENTRYPOINT\n",
    "# ============================================================\n",
    "\n",
    "def respond(inputs):\n",
    "    return conditional_chain.invoke(inputs)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. INTERACTIVE TESTING WITH input()\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"=== Tutor System Running ===\")\n",
    "    print(\"Ask a question to the teacher, or type 'test' to grade an answer.\\n\")\n",
    "\n",
    "    mode = input(\"Mode (teach/test): \").strip().lower()\n",
    "\n",
    "    if mode == \"teach\":\n",
    "        question = input(\"Your question: \")\n",
    "        result = respond({\"question\": question})\n",
    "        print(\"\\n--- Teacher answer ---\")\n",
    "        print(result)\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        # √âtudiant fournit uniquement le bar√®me et sa r√©ponse\n",
    "        rubric = input(\"Bar√®me : \")\n",
    "\n",
    "        # --- Etape 1 : Generate question using RAG context ---\n",
    "        generated_question = generate_test_question(rubric)\n",
    "        print(\"\\n--- Question g√©n√©r√©e automatiquement ---\")\n",
    "        print(generated_question)\n",
    "\n",
    "        # --- √âtape 2 : l'√©l√®ve fournit sa r√©ponse ---\n",
    "        answer = input(\"\\nR√©ponse de l'√©l√®ve : \")\n",
    "\n",
    "        # --- √âtape 3 : prompt de correction (JSON output) ---\n",
    "        scores_text = (\n",
    "            \"- Pertinence : ... /30;\\n\"\n",
    "            \"- Faits non correctes : ... /30;\\n\"\n",
    "            \"- Faits manquants : ... /30;\\n\"\n",
    "            \"- Structure : ... /10;\"\n",
    "        )\n",
    "\n",
    "        test_prompt_template = ChatPromptTemplate.from_messages([\n",
    "            (\"system\",\n",
    "            \"Act as a supportive but rigorous history teacher.\\n\"\n",
    "            \"Your goal is to evaluate the student's answer and return ONLY a JSON object.\"),\n",
    "            (\"human\",\n",
    "            \"Grading rubric: {grading_rubric}\\n\"\n",
    "            \"Question: {question}\\n\"\n",
    "            \"Answer: {answer}\\n\"\n",
    "            \"Scores template: {scores_text}\\n\"\n",
    "            \"Constraints: grade MUST equal sum of all scores.\\n\"\n",
    "            \"Return a JSON object with keys:\\n\"\n",
    "            \"- Section\\n\"\n",
    "            \"- Question\\n\"\n",
    "            \"- Answer\\n\"\n",
    "            \"- grade (0-100)\\n\"\n",
    "            \"- scores\\n\"\n",
    "            \"- advice\\n\"\n",
    "            \"No extra text or Markdown, ONLY JSON.\")\n",
    "        ])\n",
    "\n",
    "        test_chain = test_prompt_template | llm\n",
    "\n",
    "        grading_result = test_chain.invoke({\n",
    "            \"grading_rubric\": rubric,\n",
    "            \"question\": generated_question,\n",
    "            \"answer\": answer,\n",
    "            \"scores_text\": scores_text\n",
    "        })\n",
    "\n",
    "        # --- √âtape 4 : transformer la string en dictionnaire Python ---\n",
    "        import json\n",
    "\n",
    "        raw_output = grading_result.content.strip()\n",
    "\n",
    "        # Retirer les ```json ou ``` √©ventuels\n",
    "        if raw_output.startswith(\"```\"):\n",
    "            raw_output = \"\\n\".join(raw_output.split(\"\\n\")[1:-1])\n",
    "\n",
    "        try:\n",
    "            grading_json = json.loads(raw_output)\n",
    "        except json.JSONDecodeError:\n",
    "            print(\"Erreur : le LLM n'a pas retourn√© un JSON valide\")\n",
    "            grading_json = None\n",
    "\n",
    "        # --- √âtape 5 : afficher le r√©sultat ---\n",
    "        if grading_json:\n",
    "            print(\"\\n--- JSON Grading Result ---\")\n",
    "            print(grading_json)\n",
    "            print(\"\\nNote :\", grading_json[\"grade\"])\n",
    "            print(\"Conseils :\", grading_json[\"advice\"])\n",
    "        #save_result(\"Edin\", grading_json[\"Question\"], grading_json[\"Answer\"], grading_json)\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown mode.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dc7195a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'Edin', 'Quelles √©taient les principales causes de la Premi√®re Guerre mondiale et comment ont-elles conduit √† son d√©clenchement en 1914 ?', \"L'assassination de l'archeduc Fran√ßois ferdinant\", 20.0, '{\"Pertinence\": 10, \"Faits non correctes\": 20, \"Faits manquants\": 20, \"Structure\": 0}', \"Votre r√©ponse est incompl√®te et manque de structure. Vous avez mentionn√© un √©v√©nement important, mais vous devez inclure d'autres causes majeures comme le nationalisme, l'imp√©rialisme, le militarisme et les alliances. De plus, vous devez expliquer comment ces facteurs ont conduit √† la guerre en 1914. Travaillez sur la clart√© et l'organisation de votre r√©ponse.\")\n"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# 4. Query the database\n",
    "# -------------------------------\n",
    "cursor.execute(\"SELECT * FROM student_results\")\n",
    "rows = cursor.fetchall()\n",
    "for row in rows:\n",
    "    print(row)\n",
    "\n",
    "# Close connection when done\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9353a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Pertinence': 0, 'Faits non correctes': 30, 'Faits manquants': 30, 'Structure': 10}\n"
     ]
    }
   ],
   "source": [
    "print(grading_json['scores'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
