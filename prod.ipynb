{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27e5d56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load API KEY information\n",
    "load_dotenv(override=True)\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5518043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters and ChatMistral object creation\n",
    "\n",
    "\n",
    "# Create the ChatMistralAI object\n",
    "llm = ChatMistralAI(\n",
    "    temperature=1,  # Low temperature for more focused responses\n",
    "    model=\"mistral-small-latest\", \n",
    ")\n",
    "\n",
    "#If we want to understand pictures, we should use this model : \"pixtral-12b-2409\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f74b0f",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84096225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the document: 308\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Documents\n",
    "loader = PyMuPDFLoader(\"Data/Atlas.pdf\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of pages in the document: {len(docs)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58adbe3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split chunks: 696\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split Documents\n",
    "custom_separators = [\n",
    "    \"\\n \\n\",        # paragraphs\n",
    "    \"\\n\",         # lines\n",
    "    \". \",         # sentence-ish boundary\n",
    "    \"; \",         # clause boundary\n",
    "    \", \",         # phrase boundary\n",
    "    \" \",          # words\n",
    "    \"\"            # fallback: characters\n",
    "]\n",
    "text_splitter = RecursiveCharacterTextSplitter(separators = custom_separators, chunk_size=500, chunk_overlap=50)      #Paramètre à modifier par la suite pour de meilleur performance\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"Number of split chunks: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b469c310",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Generate Embeddings\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc73c795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Create and Save the Database\n",
    "# Create a vector store\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "print(\"Vector store created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69a7523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create Retriever\n",
    "# Search and retrieve information contained in the documents\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b39fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Create Prompt\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb16f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Setup LLM\n",
    "llm = ChatMistralAI(model=\"mistral-small-latest\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45f6c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Create Chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd5bb7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je ne sais pas.\n",
      "-----\n",
      "D'après le contexte fourni, l'Islande est le premier pays à avoir légalisé l'avortement, dès 1934.\n"
     ]
    }
   ],
   "source": [
    "#Exemple\n",
    "\n",
    "# Run Chain\n",
    "# Input a query about the document and print the response\n",
    "question = \"Qui a gagné le ballon d'or en 2009\"\n",
    "response = chain.invoke(question)\n",
    "print(response)\n",
    "print(\"-----\")\n",
    "question = \"Quel pays a légalisé l'avortement en premier dans le monde ?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614d87cd",
   "metadata": {},
   "source": [
    "# Prompt engineering\n",
    "\n",
    "Here we setup a basic template for our prompt engineering.\n",
    "In our case, the LLM will be a specialist in geography, in secondary school.\n",
    "\n",
    "The student will interact with the LLM in two different ways :\n",
    "    -He can ask any type of question about any topic in the course.\n",
    "    -He can ask to have his knowledge tested (he will then receive a score on his answer and a feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a7c978e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persona prompt, specific for when the student has a question about a specific part of the course\n",
    "\n",
    "persona_template = (\n",
    "    \"Act as a supportive but rigorous geography teacher.\\n\" \\\n",
    "    \"Your tone should be constructive, specific, and pedagogical.\\n\" \\\n",
    "        \"\"\"Tu es un professeur de géographie avec 20 ans d'expérience, et ton but est de répondre aux questions d'un élève en difficulté.\n",
    "            Tu es encourageant, mais tout de fois rigoureux quant à la précision de tes réponses.\n",
    "\n",
    "    CONTRAINTES:\n",
    "    1. Utilise UNIQUEMENT le contexte fourni qui vient d .\n",
    "    2. Cite chaque fait avec la page sous forme [Page X].\n",
    "    3. Ne fabrique rien.\n",
    "\n",
    "    Format attendu:\n",
    "    Réponse concise en français.\n",
    "    CITES: Page: X,Y,... (liste unique de pages utilisées)\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Contexte:\n",
    "    {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "scores = \"\"\"\n",
    "- Pertinence : Est-ce que l'étudiant répond bien à la question posé et non pas à autre chose  /30;\n",
    "- Faits non correctes: Est-ce qu'il y'a des faits qui ne sont pas correctes dans la réponse  /30;\n",
    "- Faits manquants : Est-ce que tous les faits attendus sont bien présent dans la réponse  /30;\n",
    "- Stucture : Est-ce que la réponse est bien stucturée /10;\n",
    "\"\"\"\n",
    "\n",
    "test_template = (    f\"Act as a supportive but rigorous history teacher.\\n\"\n",
    "    \"Your goal is to generate a question based on the course.\"             \n",
    "    \"The student gives you an answer and your goal is to evaluate it.\\n\"\n",
    "    \"Assignment requirement: {task_description}\\n\"\n",
    "    \"Grading rubric: {grading_rubric}\\n\"\n",
    "    \"Return ONLY a JSON object with these keys:\\n\"\n",
    "    \"- Section: the general theme of the question\\n\"\n",
    "    \"- Question: the question you asked the student\\n\"\n",
    "    \"- Answer: The answer the student gave\\n\"\n",
    "    \"- grade: number (0-100), must equal sum of all scores\\n\"\n",
    "    \"- scores: f{scores} \\n\"\n",
    "    \"- advice: array of short, actionable improvement suggestions\\n\"\n",
    "    \"Constraints: grade MUST equal Pertinence+Faits non correctes + Faits manquants + Structure. No extra text outside the JSON.\\n\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c5b8855",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VectorStoreRetriever' object has no attribute 'get_relevant_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 129\u001b[39m\n\u001b[32m    122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m conditional_chain.invoke(inputs)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# EXAMPLES\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    127\u001b[39m \n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# Persona mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m result1 = \u001b[43mrespond\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mQuelle est la différence entre climat continental et climat océanique ?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    131\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[38;5;28mprint\u001b[39m(result1)\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Test mode\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 122\u001b[39m, in \u001b[36mrespond\u001b[39m\u001b[34m(inputs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mscores_text\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[32m    121\u001b[39m     inputs[\u001b[33m\"\u001b[39m\u001b[33mscores_text\u001b[39m\u001b[33m\"\u001b[39m] = scores\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconditional_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\branch.py:224\u001b[39m, in \u001b[36mRunnableBranch.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    215\u001b[39m     expression_value = condition.invoke(\n\u001b[32m    216\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    217\u001b[39m         config=patch_config(\n\u001b[32m   (...)\u001b[39m\u001b[32m    220\u001b[39m         ),\n\u001b[32m    221\u001b[39m     )\n\u001b[32m    223\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m expression_value:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m         output = \u001b[43mrunnable\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbranch:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43m+\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    232\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3127\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3127\u001b[39m         input_ = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3128\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3129\u001b[39m         input_ = context.run(step.invoke, input_, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3853\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3848\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3849\u001b[39m         futures = [\n\u001b[32m   3850\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3851\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3852\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3853\u001b[39m         output = \u001b[43m{\u001b[49m\n\u001b[32m   3854\u001b[39m \u001b[43m            \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3855\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3856\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m   3857\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3854\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m   3848\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3849\u001b[39m         futures = [\n\u001b[32m   3850\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3851\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3852\u001b[39m         ]\n\u001b[32m   3853\u001b[39m         output = {\n\u001b[32m-> \u001b[39m\u001b[32m3854\u001b[39m             key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3855\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   3856\u001b[39m         }\n\u001b[32m   3857\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3858\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3837\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input_, config, key)\u001b[39m\n\u001b[32m   3831\u001b[39m child_config = patch_config(\n\u001b[32m   3832\u001b[39m     config,\n\u001b[32m   3833\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3834\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3835\u001b[39m )\n\u001b[32m   3836\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3837\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m context.run(\n\u001b[32m   3838\u001b[39m         step.invoke,\n\u001b[32m   3839\u001b[39m         input_,\n\u001b[32m   3840\u001b[39m         child_config,\n\u001b[32m   3841\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4857\u001b[39m, in \u001b[36mRunnableLambda.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   4842\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Invoke this `Runnable` synchronously.\u001b[39;00m\n\u001b[32m   4843\u001b[39m \n\u001b[32m   4844\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4854\u001b[39m \n\u001b[32m   4855\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfunc\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m4857\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4858\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4859\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4860\u001b[39m \u001b[43m        \u001b[49m\u001b[43mensure_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4861\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4862\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4863\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mCannot invoke a coroutine function synchronously.Use `ainvoke` instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4864\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2050\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   2046\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   2047\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   2048\u001b[39m         output = cast(\n\u001b[32m   2049\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   2052\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2053\u001b[39m \u001b[43m                \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2055\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2056\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   2058\u001b[39m         )\n\u001b[32m   2059\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2060\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\base.py:4714\u001b[39m, in \u001b[36mRunnableLambda._invoke\u001b[39m\u001b[34m(self, input_, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m   4712\u001b[39m                 output = chunk\n\u001b[32m   4713\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4714\u001b[39m     output = \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4715\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   4716\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4717\u001b[39m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[32m   4718\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 81\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(inp)\u001b[39m\n\u001b[32m     54\u001b[39m test_template = PromptTemplate(\n\u001b[32m     55\u001b[39m     input_variables=[\u001b[33m\"\u001b[39m\u001b[33mtask_description\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mgrading_rubric\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mscores_text\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     56\u001b[39m     template=(\n\u001b[32m   (...)\u001b[39m\u001b[32m     69\u001b[39m     )\n\u001b[32m     70\u001b[39m )\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# CHAINS\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m     78\u001b[39m persona_chain = (\n\u001b[32m     79\u001b[39m     {\n\u001b[32m     80\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m inp: inp[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m inp: \u001b[43mretriever\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_relevant_documents\u001b[49m(inp[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m     82\u001b[39m     }\n\u001b[32m     83\u001b[39m     | RunnableLambda(\n\u001b[32m     84\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m d: {\n\u001b[32m     85\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m: d[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     86\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m d[\u001b[33m\"\u001b[39m\u001b[33mcontext\u001b[39m\u001b[33m\"\u001b[39m]])\n\u001b[32m     87\u001b[39m         }\n\u001b[32m     88\u001b[39m     )\n\u001b[32m     89\u001b[39m     | persona_template\n\u001b[32m     90\u001b[39m     | llm\n\u001b[32m     91\u001b[39m     | StrOutputParser()\n\u001b[32m     92\u001b[39m )\n\u001b[32m     94\u001b[39m test_chain = (\n\u001b[32m     95\u001b[39m     test_template\n\u001b[32m     96\u001b[39m     | llm\n\u001b[32m     97\u001b[39m     | StrOutputParser()\n\u001b[32m     98\u001b[39m )\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# ROUTING LOGIC\u001b[39;00m\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# -------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Comor\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydantic\\main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'VectorStoreRetriever' object has no attribute 'get_relevant_documents'"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableBranch, RunnableLambda\n",
    "\n",
    "# -------------------------\n",
    "# LLM SETUP\n",
    "# -------------------------\n",
    "\n",
    "# Standard text model\n",
    "llm = ChatMistralAI(\n",
    "    temperature=1,\n",
    "    model=\"mistral-small-latest\",\n",
    ")\n",
    "\n",
    "# If you ever need vision:\n",
    "# llm = ChatMistralAI(\n",
    "#     temperature=1,\n",
    "#     model=\"pixtral-12b-2409\"\n",
    "# )\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# -------------------------\n",
    "# PROMPTS\n",
    "# -------------------------\n",
    "\n",
    "persona_template = PromptTemplate(\n",
    "    input_variables=[\"question\", \"context\"],\n",
    "    template=(\n",
    "        \"Act as a supportive but rigorous geography teacher.\\n\"\n",
    "        \"Ton but est de répondre aux questions d'un élève en difficulté.\\n\"\n",
    "        \"Tu es encourageant, mais rigoureux dans la précision.\\n\\n\"\n",
    "        \"CONTRAINTES:\\n\"\n",
    "        \"1. Utilise UNIQUEMENT le contexte fourni.\\n\"\n",
    "        \"2. Cite chaque fait avec la page sous forme [Page X].\\n\"\n",
    "        \"3. Ne fabrique rien.\\n\\n\"\n",
    "        \"Format attendu:\\n\"\n",
    "        \"Réponse concise en français.\\n\"\n",
    "        \"CITES: Page: X,Y,...\\n\\n\"\n",
    "        \"Question: {question}\\n\\n\"\n",
    "        \"Contexte:\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "scores = \"\"\"\n",
    "- Pertinence : ... /30;\n",
    "- Faits non correctes : ... /30;\n",
    "- Faits manquants : ... /30;\n",
    "- Stucture : ... /10;\n",
    "\"\"\"\n",
    "\n",
    "test_template = PromptTemplate(\n",
    "    input_variables=[\"task_description\", \"grading_rubric\", \"question\", \"answer\", \"scores_text\"],\n",
    "    template=(\n",
    "        \"Act as a supportive but rigorous history teacher.\\n\"\n",
    "        \"Your goal is to evaluate the student's answer.\\n\"\n",
    "        \"Assignment requirement: {task_description}\\n\"\n",
    "        \"Grading rubric: {grading_rubric}\\n\"\n",
    "        \"Return ONLY a JSON object with these keys:\\n\"\n",
    "        \"- Section\\n\"\n",
    "        \"- Question\\n\"\n",
    "        \"- Answer\\n\"\n",
    "        \"- grade\\n\"\n",
    "        \"- scores: {scores_text}\\n\"\n",
    "        \"- advice\\n\"\n",
    "        \"Constraints: grade MUST equal Pertinence + Faits non correctes + Faits manquants + Structure.\\n\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# CHAINS\n",
    "# -------------------------\n",
    "\n",
    "persona_chain = (\n",
    "    {\n",
    "        \"question\": lambda inp: inp[\"question\"],\n",
    "        \"context\": lambda inp: retriever.get_relevant_documents(inp[\"question\"]),\n",
    "    }\n",
    "    | RunnableLambda(\n",
    "        lambda d: {\n",
    "            \"question\": d[\"question\"],\n",
    "            \"context\": \"\\n\\n\".join([doc.page_content for doc in d[\"context\"]])\n",
    "        }\n",
    "    )\n",
    "    | persona_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "test_chain = (\n",
    "    test_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# ROUTING LOGIC\n",
    "# -------------------------\n",
    "\n",
    "def route_to_persona(inputs):\n",
    "    # If the user does NOT provide \"answer\", it's a question → persona chain\n",
    "    return \"answer\" not in inputs\n",
    "\n",
    "conditional_chain = RunnableBranch(\n",
    "    (lambda inputs: route_to_persona(inputs), persona_chain),\n",
    "    (lambda inputs: not route_to_persona(inputs), test_chain),\n",
    "    test_chain  # default\n",
    ")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# MAIN ENTRYPOINT\n",
    "# -------------------------\n",
    "\n",
    "def respond(inputs):\n",
    "    if \"scores_text\" not in inputs:\n",
    "        inputs[\"scores_text\"] = scores\n",
    "    return conditional_chain.invoke(inputs)\n",
    "\n",
    "# -------------------------\n",
    "# EXAMPLES\n",
    "# -------------------------\n",
    "\n",
    "# Persona mode\n",
    "result1 = respond({\n",
    "    \"question\": \"Quelle est la différence entre climat continental et climat océanique ?\"\n",
    "})\n",
    "print(result1)\n",
    "\n",
    "\n",
    "# Test mode\n",
    "result2 = respond({\n",
    "    \"task_description\": \"Définir la Révolution industrielle.\",\n",
    "    \"grading_rubric\": \"Les 4 critères.\",\n",
    "    \"question\": \"Explique la Révolution industrielle.\",\n",
    "    \"answer\": \"Elle commence en Angleterre grâce à la machine à vapeur.\"\n",
    "})\n",
    "print(result2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a62ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Conditional Chain Tester ===\n",
      "\n",
      "Choose mode:\n",
      "1. Ask a course question (persona prompt)\n",
      "2. Submit an answer for grading (test prompt)\n",
      "\n",
      "\n",
      "--- Response ---\n",
      "Voici une réponse concise et rigoureuse basée uniquement sur le contexte fourni :\n",
      "\n",
      "\"Le contexte ne fournit pas d'informations précises sur le début de la légalisation de l'avortement. Pour une réponse complète, il faudrait des sources supplémentaires comme des dates, des pays ou des lois spécifiques.\"\n",
      "\n",
      "CITES : Aucune page mentionnée dans le contexte fourni.\n",
      "\n",
      "*Je reste à votre disposition pour approfondir le sujet si vous fournissez des détails supplémentaires (livre, pages, etc.).*\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def interactive_test():\n",
    "    print(\"=== Conditional Chain Tester ===\\n\")\n",
    "    print(\"Choose mode:\")\n",
    "    print(\"1. Ask a question about the course (auto-retrieval)\")\n",
    "    print(\"2. Submit a student's answer for grading\\n\")\n",
    "\n",
    "    mode = input(\"Your choice (1/2): \").strip()\n",
    "\n",
    "    if mode == \"1\":\n",
    "        question = input(\"\\nEnter your question: \")\n",
    "        inputs = {\"question\": question}\n",
    "\n",
    "        print(\"\\n--- Response ---\")\n",
    "        print(respond(inputs))\n",
    "        print(\"----------------\\n\")\n",
    "\n",
    "    elif mode == \"2\":\n",
    "        task_description = input(\"\\nTask description: \")\n",
    "        grading_rubric = input(\"Grading rubric: \")\n",
    "        question = input(\"The question asked to the student: \")\n",
    "        answer = input(\"Student answer: \")\n",
    "\n",
    "        inputs = {\n",
    "            \"task_description\": task_description,\n",
    "            \"grading_rubric\": grading_rubric,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"scores_text\": scores,\n",
    "        }\n",
    "\n",
    "        print(\"\\n--- Correction ---\")\n",
    "        print(respond(inputs))\n",
    "        print(\"------------------\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"Invalid option.\\n\")\n",
    "\n",
    "\n",
    "# Launch interactive mode\n",
    "interactive_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e546a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tutor System Running ===\n",
      "Ask a question to the teacher, or type 'test' to grade an answer.\n",
      "\n",
      "\n",
      "--- Grading result ---\n",
      "content=\"**Note : 20/20**\\n\\n**Justification :**\\nLa réponse de l'élève est correcte et précise. L'URSS est effectivement le premier pays à avoir légalisé l'avortement en 1920. La réponse est concise et directe, répondant parfaitement à la question posée. Il n'y a pas d'erreurs factuelles ou grammaticales à relever. La réponse mérite donc la note maximale.\" additional_kwargs={} response_metadata={'token_usage': {'prompt_tokens': 87, 'total_tokens': 176, 'completion_tokens': 89}, 'model_name': 'mistral-small-latest', 'model': 'mistral-small-latest', 'finish_reason': 'stop', 'model_provider': 'mistralai'} id='lc_run--caf55dd5-76fe-4609-8e1d-6fb7cfb417b2-0' usage_metadata={'input_tokens': 87, 'output_tokens': 89, 'total_tokens': 176}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL WORKING VERSION — RAG + PERSONA CHAIN + GRADING CHAIN\n",
    "# ============================================================\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI, MistralAIEmbeddings\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnableBranch\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1. LLM CONFIG\n",
    "# ============================================================\n",
    "\n",
    "llm = ChatMistralAI(\n",
    "    model=\"mistral-small-latest\",   # Vision model would be: pixtral-12b-2409\n",
    "    temperature=1\n",
    ")\n",
    "\n",
    "# OPTIONAL: if you want vision, replace above with:\n",
    "# llm = ChatMistralAI(model=\"pixtral-12b-2409\", temperature=1)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2. LOAD DOCUMENTS (PDF)\n",
    "# ============================================================\n",
    "\n",
    "loader = PyMuPDFLoader(\"Data/Atlas.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3. SPLIT DOCUMENTS\n",
    "# ============================================================\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\n",
    "        \"\\nCHAPITRE\", \"\\n##\", \"\\n###\", \"\\nSection\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4. EMBEDDINGS + VECTORSTORE + RETRIEVER\n",
    "# ============================================================\n",
    "\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(split_documents, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5. PERSONA CHAIN (Teacher mode + RAG)\n",
    "# ============================================================\n",
    "\n",
    "persona_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Tu es un professeur bienveillant. Explique simplement mais sans infantiliser. \"\n",
    "     \"Appuie-toi uniquement sur le contexte fourni.\"),\n",
    "    (\"human\",\n",
    "     \"Question: {question}\\n\\n\"\n",
    "     \"Contexte issu des documents:\\n{context}\")\n",
    "])\n",
    "\n",
    "persona_chain = (\n",
    "    persona_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 6. GRADING CHAIN (Automatic evaluation)\n",
    "# ============================================================\n",
    "\n",
    "test_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"Tu es un correcteur automatique. Évalue la réponse de l'élève selon les critères fournis.\"),\n",
    "    (\"human\",\n",
    "     \"Instruction donnée à l'élève : {task_description}\\n\"\n",
    "     \"Barème : {grading_rubric}\\n\"\n",
    "     \"Question : {question}\\n\"\n",
    "     \"Réponse de l'élève : {answer}\\n\\n\"\n",
    "     \"Donne une note sur 20 + justification.\")\n",
    "])\n",
    "\n",
    "test_chain = (\n",
    "    test_template\n",
    "    | llm\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 7. RAG WRAPPER — run retrieval only if needed\n",
    "# ============================================================\n",
    "\n",
    "def rag_logic(inputs):\n",
    "    query = inputs.get(\"question\", \"\")\n",
    "    if not query:\n",
    "        inputs[\"context\"] = \"\"\n",
    "        return inputs\n",
    "\n",
    "    docs = retriever.invoke(query)\n",
    "    ctx = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    inputs[\"context\"] = ctx\n",
    "    return inputs\n",
    "\n",
    "\n",
    "rag_chain = RunnableLambda(rag_logic)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 8. ROUTING — decide whether to use Persona or Test chain\n",
    "# ============================================================\n",
    "\n",
    "def route(inputs):\n",
    "    \"\"\"\n",
    "    If 'answer' is provided → grading mode.\n",
    "    Otherwise → persona/teacher mode.\n",
    "    \"\"\"\n",
    "    return \"answer\" not in inputs\n",
    "\n",
    "\n",
    "conditional_chain = RunnableBranch(\n",
    "    # condition → persona mode\n",
    "    (\n",
    "        lambda inputs: route(inputs),\n",
    "        rag_chain | persona_chain\n",
    "    ),\n",
    "    # fallback → grading mode\n",
    "    test_template | llm\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 9. MAIN ENTRYPOINT\n",
    "# ============================================================\n",
    "\n",
    "def respond(inputs):\n",
    "    return conditional_chain.invoke(inputs)\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 10. INTERACTIVE TESTING WITH input()\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"=== Tutor System Running ===\")\n",
    "    print(\"Ask a question to the teacher, or type 'test' to grade an answer.\\n\")\n",
    "\n",
    "    mode = input(\"Mode (teach/test): \").strip().lower()\n",
    "\n",
    "    if mode == \"teach\":\n",
    "        question = input(\"Your question: \")\n",
    "        result = respond({\"question\": question})\n",
    "        print(\"\\n--- Teacher answer ---\")\n",
    "        print(result)\n",
    "\n",
    "    elif mode == \"test\":\n",
    "        task = input(\"Description de la consigne : \")\n",
    "        rubric = input(\"Barème : \")\n",
    "        question = input(\"Question posée : \")\n",
    "        answer = input(\"Réponse de l'élève : \")\n",
    "\n",
    "        result = respond({\n",
    "            \"task_description\": task,\n",
    "            \"grading_rubric\": rubric,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer\n",
    "        })\n",
    "\n",
    "        print(\"\\n--- Grading result ---\")\n",
    "        print(result)\n",
    "\n",
    "    else:\n",
    "        print(\"Unknown mode.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
